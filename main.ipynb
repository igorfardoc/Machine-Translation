{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vocabulary import *\n",
    "from DataLoader import *\n",
    "from Model import *\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk import bleu_score\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "from random import shuffle\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('files'):\n",
    "    os.makedirs('files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = []\n",
    "dst = []\n",
    "with open('data/train.tags.de-en.de') as f:\n",
    "    src = list(map(lambda x: x.lower(), f.read().split('\\n')))[1:]\n",
    "with open('data/train.tags.de-en.en') as f:\n",
    "    dst = list(map(lambda x: x.lower(), f.read().split('\\n')))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "src = list(map(lambda x: ' '.join(tokenizer.tokenize(x)), src))\n",
    "dst = list(map(lambda x: ' '.join(tokenizer.tokenize(x)), dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/src.txt', 'w') as f:\n",
    "    f.write('\\n'.join(src))\n",
    "with open('files/dst.txt', 'w') as f:\n",
    "    f.write('\\n'.join(dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/src.txt', 'r') as f_in, open('files/bpe_src.rules', 'w') as f_out:\n",
    "    learn_bpe(f_in, f_out, num_symbols=4000, verbose=False)\n",
    "with open('files/dst.txt', 'r') as f_in, open('files/bpe_dst.rules', 'w') as f_out:\n",
    "    learn_bpe(f_in, f_out, num_symbols=4000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/bpe_src.rules') as f:\n",
    "    bpe_src = BPE(f, separator='~@~@')\n",
    "with open('files/bpe_dst.rules') as f:\n",
    "    bpe_dst = BPE(f, separator='~@~@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = list(map(lambda x: ' '.join(bpe_src.segment(x).split(' ')), src))\n",
    "dst = list(map(lambda x: ' '.join(bpe_dst.segment(x).split(' ')), dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_src = Voc(src)\n",
    "voc_dst = Voc(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dst = list(map(lambda x: (src[x], dst[x]), range(len(src))))\n",
    "loader = DataLoader(src_dst, voc_src, voc_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(voc_src, 128, 512, 512)\n",
    "dec = DecoderStep(voc_dst, 128, 512)\n",
    "trainable_weights = enc.trainable_weights + dec.trainable_weights\n",
    "optimizer = tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(x, y):\n",
    "    \"\"\"\n",
    "    Compute mean crossentropy over all time-steps, perform gradient descent\n",
    "    :param x: input tokens, tensor of int32[batch_size, input_length]\n",
    "    :param y: output tokens, tensor of int32[batch_size, output_length]\n",
    "    :returns: mean loss funtion\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(trainable_weights)\n",
    "        \n",
    "        losses = []\n",
    "        states = enc(x)\n",
    "        res = tf.zeros([x.shape[0], dec.hid_size])\n",
    "        for i in range(y.shape[1] - 1):\n",
    "            res, prob, state_probs = dec(y[:, i], res, states)\n",
    "            losses.append(tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y[:, i + 1], prob)))\n",
    "\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        \n",
    "    gradients = tape.gradient(mean_loss, trainable_weights)\n",
    "    optimizer.apply_gradients([(gradients[i], trainable_weights[i]) for i in range(len(gradients))])\n",
    "    return float(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_states(states, symbols=10, until_eos=False):\n",
    "    state = tf.zeros([states.shape[0], dec.hid_size])\n",
    "    tokens = tf.convert_to_tensor([voc_dst.token_to_num['_BOS_']])\n",
    "    str_res = ''\n",
    "    if until_eos:\n",
    "        while True:\n",
    "            tate, next_probs, state_probs = dec(tokens, state, states)\n",
    "            tokens = tf.argmax(next_probs, axis=-1)\n",
    "            if str(voc_dst.num_to_token[int(tokens[0])]) == '_EOS_':\n",
    "                break\n",
    "            str_res += str(voc_dst.num_to_token[int(tokens[0])]) + ' '\n",
    "    else:\n",
    "        for i in range(symbols):\n",
    "            state, next_probs, state_probs = dec(tokens, state, states)\n",
    "            tokens = tf.argmax(next_probs, axis=-1)\n",
    "            if str(voc_dst.num_to_token[int(tokens[0])]) == '_EOS_':\n",
    "                break\n",
    "            str_res += str(voc_dst.num_to_token[int(tokens[0])]) + ' '\n",
    "    if len(str_res) > 0:\n",
    "        return str_res[:-1]\n",
    "    return str_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random(symbols=10, until_eos=False):\n",
    "    x_batch, y_batch = loader.get_train_batch(1)\n",
    "    print('SOURCE:')\n",
    "    print(voc_src.tensor_to_strings(x_batch)[0])\n",
    "    print('REFERENCE:')\n",
    "    print(voc_dst.tensor_to_strings(y_batch)[0])\n",
    "    print('PREDICTION:')\n",
    "    states = enc(x_batch)\n",
    "    print(predict_by_states(states, symbols=symbols, until_eos=until_eos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_string(string, symbols=10, until_eos=False):\n",
    "    string = ' '.join(tokenizer.tokenize(string.lower()))\n",
    "    string = bpe_src.segment(string)\n",
    "    x = voc_src.strings_to_tensor([string])\n",
    "    states = enc(x)\n",
    "    print(predict_by_states(states, symbols=symbols, until_eos=until_eos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_on_test(iterations=5000):\n",
    "    references = []\n",
    "    translations = []\n",
    "    for i in range(iterations):\n",
    "        x, y = loader.get_test_batch(1)\n",
    "        references.append(voc_dst.tensor_to_strings(y)[0])\n",
    "        states = enc(x)\n",
    "        translations.append(predict_by_states(states, symbols=y.shape[1] * 2))\n",
    "    return bleu_score.corpus_bleu([[ref] for ref in references], translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, iterations, interval_of_saving_losses=0, show_losses=False, show_traslation=False):\n",
    "    batch_losses = []\n",
    "    for i in range(iterations):\n",
    "        x_batch, y_batch = loader.get_train_batch(batch_size)\n",
    "        loss = train_on_batch(x_batch, y_batch)\n",
    "        batch_losses.append(loss)\n",
    "    \n",
    "        if show_losses:\n",
    "            clear_output(True)\n",
    "            plt.plot(batch_losses)\n",
    "            plt.show()\n",
    "            \n",
    "        if interval_of_saving_losses != 0:\n",
    "            if i % interval_of_saving_losses == 0:\n",
    "                plt.plot(batch_losses)\n",
    "                plt.savefig('files/losses.png')\n",
    "            \n",
    "        if i % 100 == 0 and show_traslation:\n",
    "            predict_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(32, 1000000000, interval_of_saving_losses=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f942f9a49a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_on_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1ca06929f920>\u001b[0m in \u001b[0;36maccuracy_on_test\u001b[0;34m(iterations)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_dst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_to_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_on_test(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_prev_tokens = L.Input(shape=[], dtype=tf.int32)\n",
    "decoder_prev_state = L.Input(shape=[dec.hid_size], dtype=tf.float32)\n",
    "\n",
    "new_state, _ = dec.rnn(dec.emb(decoder_prev_tokens), decoder_prev_state)\n",
    "new_probs = dec.prob(new_state)\n",
    "decoder_model = tf.keras.Model(inputs=[decoder_prev_tokens, decoder_prev_state], \n",
    "                               outputs=dec.rnn(dec.emb(decoder_prev_tokens), decoder_prev_state))\n",
    "\n",
    "#     def call(self, prev_tokens, prev_state):\n",
    "#         # prev_tokens: [batch_size] of int32\n",
    "#         # prev_state: [batch_size, hid_size], same as returned by encoder\n",
    "        \n",
    "        \n",
    "#         new_state, _ = self.rnn(self.emb(prev_tokens), prev_state)\n",
    "#         # ^-- [batch_size, hid_size]\n",
    "        \n",
    "#         new_token_probs = self.prob(new_state)\n",
    "#         # ^-- [batch_size, len(voc)]\n",
    "        \n",
    "#         return new_state, new_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-ec9c498f3f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflowjs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfjs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfjs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'enc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflowjs/converters/keras_h5_conversion.py\u001b[0m in \u001b[0;36msave_keras_model\u001b[0;34m(model, artifacts_dir, quantization_dtype_map, weight_shard_size_bytes)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \"\"\"\n\u001b[1;32m    340\u001b[0m   \u001b[0mtemp_h5_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmktemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_h5_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m   topology_json, weight_groups = (\n\u001b[1;32m    343\u001b[0m       h5_merged_saved_model_to_tfjs_format(temp_h5_path))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1976\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[0;32m-> 1978\u001b[0;31m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[1;32m   1979\u001b[0m                     signatures, options)\n\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    128\u001b[0m           \u001b[0;34m'to the Tensorflow SavedModel format (by setting save_format=\"tf\") '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m--> 130\u001b[0;31m     hdf5_format.save_model_to_hdf5(\n\u001b[0m\u001b[1;32m    131\u001b[0m         model, filepath, overwrite, include_optimizer)\n\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;31m# entities like metrics added using `add_metric` and losses added using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;31m# `add_loss.`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_undeduplicated_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     logging.warning('Found duplicated `Variable`s in Model\\'s `weights`. '\n\u001b[1;32m     90\u001b[0m                     \u001b[0;34m'This is usually caused by `Variable`s being shared by '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mweights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2318\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     \"\"\"\n\u001b[0;32m-> 2320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_undeduplicated_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_undeduplicated_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2323\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_undeduplicated_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m     \u001b[0;34m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2326\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;31m# When the graph has not been initialized, use the Model's implementation to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;31m# to check if the weights has been created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=bad-super-call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2446\u001b[0m       \u001b[0;31m# been invoked yet, this will cover both sequential and subclass model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2447\u001b[0m       \u001b[0;31m# Also make sure to exclude Model class itself which has build() defined.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2448\u001b[0;31m       raise ValueError('Weights for model %s have not yet been created. '\n\u001b[0m\u001b[1;32m   2449\u001b[0m                        \u001b[0;34m'Weights are created when the Model is first called on '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2450\u001b[0m                        \u001b[0;34m'inputs or `build()` is called with an `input_shape`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(encode_model, 'enc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
